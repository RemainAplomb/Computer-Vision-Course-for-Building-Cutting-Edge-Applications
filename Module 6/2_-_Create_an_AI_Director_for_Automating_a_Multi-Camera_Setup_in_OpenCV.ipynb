{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<center><font style=\"color:rgb(100,109,254)\">Module 6: AI Video Director For Automating Multi-Camera Setup</font> </center>**\n",
    "\n",
    "<center>\n",
    "    <img src='https://drive.google.com/uc?export=download&id=19tHZtvNS8ot5c9jvsbjPRk2SkI-8_1_Z' width=800> \n",
    "</center>\n",
    "    \n",
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\"> Module Outline </font>**\n",
    "\n",
    "The module can be split into the following parts:\n",
    "\n",
    "- *Lesson 1: Extract Eyes and Nose Keypoints*\n",
    "\n",
    "- ***Lesson 2:* Create an AI Director for Automating a Multi-Camera Setup in OpenCV** *(This Tutorial)*\n",
    "\n",
    "- *Lesson 3: Utilize the AI Director for Automating a Multi-Camera Setup in OBS*\n",
    "\n",
    "\n",
    "**Please Note**, these Jupyter Notebooks are not for sharing; do read the Copyright message below the Code License Agreement section which is in the last cell of this notebook.\n",
    "-Taha Anwar\n",
    "\n",
    "Alright, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\"> Import the Libraries</font>**\n",
    "\n",
    "First, we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mediapipe version: 0.8.10.1, it should be 0.8.9.1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from previous_lesson import detectFacialLandmarks, getFaceKeypoints\n",
    "from importlib.metadata import version\n",
    "print(f\"Mediapipe version: {version('mediapipe')}, it should be 0.8.9.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Initializations</font>**\n",
    "\n",
    "After that, in this step, we will perform all the initializations required to build the application.\n",
    "\n",
    "### **<font style=\"color:rgb(134,19,348)\">Cameras Indexes List</font>**\n",
    "\n",
    "So first, we will have to initialize a list containing the indexes of the cameras that we want to use in the application. Obviously, You should have at least two webcams to test this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize  a list to store the indexes of the cameras.\n",
    "CAMERAS_INDEXES = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Face Landmarks Detection Model</font>**\n",
    "\n",
    "\n",
    "After that, we will have to initialize the **`mp.solutions.face_mesh`** class and then set up the **`mp.solutions.face_mesh.FaceMesh()`** function with appropriate arguments (for each webcam), as we have been doing in the previous lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe face mesh class.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Initialize a list to store the facemesh functions for different webcam feeds.\n",
    "facemesh_functions = []\n",
    "\n",
    "# Iterate over the number of times equal to the number of cameras.\n",
    "for i in range(len(CAMERAS_INDEXES)):\n",
    "    \n",
    "    # Setup the face landmarks function for the camera.\n",
    "    facemesh_functions.append(mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, \n",
    "                                                    refine_landmarks=True, \n",
    "                                                    min_detection_confidence=0.5,\n",
    "                                                    min_tracking_confidence=0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Create a Function to Calculate Head Pose Score</font>**\n",
    "\n",
    "Now we will create a function **`getHeadScore()`**, that will utilize the nose and eyes landmarks to calculate the difference between the nose tip landmark and the mid-point between the eyes center landmarks. This difference will be the lowest for the camera towards which the person will be looking, so we will call this difference as head pose score which will help us to automate switching between cameras in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeadScore(keypoints):\n",
    "    '''\n",
    "    This function calculates the difference between the nose tip and both eyes mid-point.\n",
    "    Args:\n",
    "        keypoints: A tuple containing the nose, left eye center, right eye center landmarks.\n",
    "    Returns:\n",
    "        difference_norm: The normalized difference between the nose tip and both eyes mid-point.\n",
    "    '''\n",
    "    \n",
    "    # Get the nose tip, left eye center, and right eye center keypoints.\n",
    "    nose_tip, left_eye_center, right_eye_center = keypoints\n",
    "    \n",
    "    # Get the x-coordinates of the nose tip, left eye center, and right eye center keypoints.\n",
    "    nose_x, _ = nose_tip\n",
    "    left_eye_x, _ = left_eye_center\n",
    "    right_eye_x, _ = right_eye_center\n",
    "    \n",
    "    # Calculate the mid-point of the x-coordinates of the left eye center, and right eye center.\n",
    "    mid_x = (left_eye_x + right_eye_x)/2\n",
    "    \n",
    "    # Get the difference betweeen the x-coordinates of the nose tip\n",
    "    # and mid-point of the left eye center, and right eye center.\n",
    "    difference = abs(nose_x - mid_x)\n",
    "    \n",
    "    # Get the x-coordinate distance between the left eye center, and right eye center.\n",
    "    eyes_distance_x = abs(left_eye_x - right_eye_x)\n",
    "            \n",
    "    # Normalize the difference by dividing it with the distance between the left, and right eye.\n",
    "    # This is done so that the difference in distance of different cameras \n",
    "    # from the person does not effect the score (difference).\n",
    "    difference_norm = difference / eyes_distance_x\n",
    "    \n",
    "    # Return the normalized difference.\n",
    "    return difference_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will utilize the function **`getHeadScore()`** created above, to get the head pose score (difference) for each camera and will select the camera towards which the person in the feed is looking. Note that each camera feed should only have one same person in it to get this application to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the VideoCapture objects of different webcams.\n",
    "cameras_readers = []\n",
    "\n",
    "# Iterate over the indexes of the cameras.\n",
    "for camera_id, camera_index in enumerate(CAMERAS_INDEXES):\n",
    "    \n",
    "    # Append a VideoCapture object into the list.\n",
    "    #cameras_readers.append(cv2.VideoCapture(camera_index))\n",
    "    cameras_readers.append(cv2.VideoCapture(camera_index))\n",
    "\n",
    "    # Set the webcam feed width and height.\n",
    "    cameras_readers[camera_id].set(3,1280)\n",
    "    cameras_readers[camera_id].set(4,960)\n",
    "    \n",
    "    # Create a named window for resizing purposes.\n",
    "    cv2.namedWindow(f'Camera {camera_id}', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# This Will Make the Window have the same size for all cameras.\n",
    "win_name =  'Selected Camera Video'\n",
    "cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(win_name, 1600, 1000)\n",
    "   \n",
    "# Iterate until a termination (break) statement is executed.\n",
    "while True:\n",
    "    \n",
    "    # Initialize a list to store a frame of the webcam towards which user is looking.\n",
    "    frame_to_show = []\n",
    "    \n",
    "    # Initialize a variable to store the minimum score across all the webcam feeds.\n",
    "    min_score = 1000\n",
    "    \n",
    "    # Iterate over the VideoCapture objects. \n",
    "    for camera_id, camera_reader in enumerate(cameras_readers):\n",
    "        \n",
    "        # Read a frame.\n",
    "        ok, frame = camera_reader.read()\n",
    "\n",
    "        # Check if frame is not read properly then \n",
    "        # continue to the next iteration to read the next frame.\n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "        frame = cv2.flip(frame, 1)\n",
    "    \n",
    "        # Perform Face landmarks detection.\n",
    "        frame, face_landmarks = detectFacialLandmarks(frame, facemesh_functions[camera_id], \n",
    "                                                      draw=False, display=False)\n",
    "\n",
    "        # Check if the Face landmarks in the frame are detected.\n",
    "        if len(face_landmarks)>0:\n",
    "\n",
    "            # Get the nose, left eye center, and right eye center landmarks.\n",
    "            frame, keypoints = getFaceKeypoints(frame, face_landmarks, draw=False, display=False)\n",
    "            \n",
    "            # Calculate the difference between the nose tip and both eyes mid-point.\n",
    "            score = getHeadScore(keypoints)\n",
    "            \n",
    "            # Check if the calculated score is less than the minimum score.\n",
    "            if score < min_score:\n",
    "                \n",
    "                # Update the frame (to show) and the minimum score.\n",
    "                frame_to_show = frame\n",
    "                min_score = score\n",
    "        \n",
    "        # Display the frame of the webcam feed we are iterating upon.\n",
    "        cv2.imshow(f'Camera {camera_id}', frame)\n",
    "    \n",
    "    # Check if the frame (to show) variable has a valid value.\n",
    "    if len(frame_to_show) > 0:\n",
    "        \n",
    "        # Display the frame (with minimum score) of the webcam i.e., towards which user is looking.\n",
    "        cv2.imshow('Selected Camera Video', frame_to_show)\n",
    "\n",
    "    # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF    \n",
    "\n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "        \n",
    "# Iterate over the VideoCapture objects. \n",
    "for camera_reader in cameras_readers:\n",
    "    \n",
    "    # Release the VideoCapture Object.                  \n",
    "    camera_reader.release()\n",
    "\n",
    "# Close the windows.\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! working perfectly fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **<font style=\"color:rgb(255,140,0)\"> Code License Agreement </font>**\n",
    "```\n",
    "Copyright (c) 2022 Bleedai.com\n",
    "\n",
    "Feel free to use this code for your own projects commercial or noncommercial, these projects can be Research-based, just for fun, for-profit, or even Education with the exception that you’re not going to use it for developing a course, book, guide, or any other educational products.\n",
    "\n",
    "Under *NO CONDITION OR CIRCUMSTANCE* you may use this code for your own paid educational or self-promotional ventures without written consent from Taha Anwar (BleedAI.com).\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab2c83b243fb879010d169f2f59fe1d865a42357da3e2fb5ab94d633edfe058a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
