{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<center><font style=\"color:rgb(100,109,254)\">Module 1: Creating a Live Gesture Control Application</font> </center>**\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=10xY6U-4CecBuMkbONZp6bPx7BAZB6xfX'>\n",
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\"> Module Outline </font>**\n",
    "\n",
    "The module can be split into the following parts:\n",
    "\n",
    "- *Lesson 1: Introduction to hand landmark detection theory*\n",
    "\n",
    "- *Lesson 2:* Create a Hands Landmarks Detector\n",
    "\n",
    "- *Lesson 3:* Build a Hands Fingers Counter\n",
    "\n",
    "- *Lesson 4:* Build a Hand Gesture Recognizer \n",
    "\n",
    "- *Lesson 5:* Distance measurement\n",
    "\n",
    "- *Lesson 6:* Brightness, Saturation, Contrast Enhancement & Gamma Correction\n",
    "\n",
    "- ***Lesson 7:* Build the Final Application** *(This Tutorial)*\n",
    "\n",
    "**Please Note**, these Jupyter Notebooks are not for sharing; do read the Copyright message below the Code License Agreement section which is in the last cell of this notebook.\n",
    "-Taha Anwar\n",
    "\n",
    "Alright, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\"> Import the Libraries</font>**\n",
    "\n",
    "First we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "from scipy.interpolate import interp1d\n",
    "from previous_lesson import (detectHandsLandmarks, recognizeGestures,\n",
    "                             calculateDistance, changeSatValue,\n",
    "                             changeContrast, gammaCorrection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Initialize the Hands Landmarks Detection Model</font>**\n",
    "\n",
    "After that, we will need to initialize the **`mp.solutions.hands`** class and then set up the **`mp.solutions.hands.Hands()`** function with appropriate arguments and also initialize **`mp.solutions.drawing_utils`** class that is required to visualize the detected landmarks, as we have been doing in the previous lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe hands class.\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Set up the Hands functions for images and videos.\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.3)\n",
    "hands_videos = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.8)\n",
    "\n",
    "# Initialize the mediapipe drawing class.\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Build the Final Application</font>**\n",
    "\n",
    "Now finally, we will put all the functions we created in the previous lessons, together to build the final application. We will change the Brightness, Saturation, Contrast, and Gamma Correction of our webcam feed in real-time utilizing our hand gestures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Live Gesture Control Application', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Initialize the minimum brightness and saturation scale factors.\n",
    "min_brightness = min_saturation = 0\n",
    "\n",
    "# Initialize the maximum brightness and saturation scale factors.\n",
    "max_brightness = max_saturation = 3\n",
    "\n",
    "# Initialize the minimum and maximum contrast scale factors. \n",
    "min_contrast=1\n",
    "max_contrast=3\n",
    "\n",
    "# Initialize the minimum and maximum gamma scale factors.\n",
    "min_gamma=0.1\n",
    "max_gamma=3\n",
    "\n",
    "# Initialize a buffer to store recognized gestures.\n",
    "buffer = deque([], maxlen=20)\n",
    "\n",
    "# Initialize some variables to store the change state.\n",
    "change_brightness = change_saturation = change_contrast = change_gamma = False\n",
    "\n",
    "# Initialize a variable to store the current mode.\n",
    "current_mode = ''\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "   \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the height and width of the frame of the webcam video.\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    \n",
    "    # Perform Hands landmarks detection on the frame.\n",
    "    _, results = detectHandsLandmarks(frame, hands_videos, draw=True, display=False)\n",
    "    \n",
    "    # Check if the hands landmarks in the frame are detected.\n",
    "    if results.multi_hand_landmarks:\n",
    "        \n",
    "        # Perform left hand gesture recognition.\n",
    "        current_gesture, size_gesture_tip_pts = recognizeGestures(frame, results,\n",
    "                                                                  hand_label='LEFT',\n",
    "                                                                  draw=False, display=False)\n",
    "        \n",
    "        # Check if a known gesture is recognized.\n",
    "        if current_gesture != 'UNKNOWN':\n",
    "            \n",
    "            # Check if all the gestures stored in the buffer are equal to the current gesture.\n",
    "            if all(current_gesture==gesture for gesture in buffer):\n",
    "                \n",
    "                # Append the current gesture into the buffer.\n",
    "                buffer.append(current_gesture)\n",
    "                \n",
    "            # Otherwise.\n",
    "            else:\n",
    "                \n",
    "                # Clear the buffer.\n",
    "                buffer.clear()\n",
    "            \n",
    "            # Check if the length of the buffer is equal to 20.\n",
    "            if len(buffer) == 20:\n",
    "                \n",
    "                # Calculate the distance between the middle finger tip and thumb tip landmark of the right hand.\n",
    "                distance = calculateDistance(frame, size_gesture_tip_pts['MIDDLE'],\n",
    "                                             size_gesture_tip_pts['THUMB'], display=False)\n",
    "                \n",
    "                # Check if the distance is calculated successfully.\n",
    "                # This will be none in case when the hand is not in the frame.\n",
    "                if distance:\n",
    "                    \n",
    "                    # Check if the current hand gesture is INDEX POINTING UP.\n",
    "                    if current_gesture == 'INDEX POINTING UP':\n",
    "                        \n",
    "                        # Get the interpolation function.\n",
    "                        brightness_interp_f = interp1d([30,230], [min_brightness, max_brightness])\n",
    "                        \n",
    "                        # Calculate the brighness scale factor based on the calculated distance.\n",
    "                        # Higher the distance, higher the brighness scale factor will be.\n",
    "                        brightness_scale = brightness_interp_f(distance)\n",
    "                        \n",
    "                        # Update the current mode and change state.\n",
    "                        current_mode = 'BRIGHTNESS'\n",
    "                        change_brightness = True\n",
    "                    \n",
    "                    # Check if the current hand gesture is VICTORY.\n",
    "                    elif current_gesture == 'VICTORY':\n",
    "                        \n",
    "                        # Get the interpolation function and calculate the saturation scale factor.\n",
    "                        saturation_interp_f = interp1d([30,230], [min_saturation, max_saturation])\n",
    "                        saturation_scale = saturation_interp_f(distance)\n",
    "                        \n",
    "                        # Update the current mode and change state.\n",
    "                        current_mode = 'SATURATION'\n",
    "                        change_saturation = True\n",
    "                    \n",
    "                    # Check if the current hand gesture is SPIDERMAN.\n",
    "                    elif current_gesture == 'PINKY':\n",
    "                        \n",
    "                        # Get the interpolation function and calculate the contrast scale factor.\n",
    "                        contrast_interp_f = interp1d([30,230], [min_contrast, max_contrast])\n",
    "                        contrast_scale = contrast_interp_f(distance)\n",
    "                        \n",
    "                        # Update the current mode and change state.\n",
    "                        current_mode = 'CONTRAST'\n",
    "                        change_contrast = True\n",
    "                    \n",
    "                    # Check if the current hand gesture is HIGH-FIVE.\n",
    "                    elif current_gesture == 'HIGH-FIVE':\n",
    "                        \n",
    "                        # Get the interpolation function and calculate the gamma scale factor.\n",
    "                        gamma_interp_f = interp1d([30,230], [min_gamma, max_gamma])\n",
    "                        gamma_scale = gamma_interp_f(distance)\n",
    "                        \n",
    "                        # Update the current mode and change state.\n",
    "                        current_mode = 'GAMMA CORRECTION'\n",
    "                        change_gamma = True\n",
    "                    \n",
    "                    # Get the interpolation function and calculate the bar value.\n",
    "                    # This will be used to draw a filled rectangle of height varying with the distance.\n",
    "                    bar_interp_f = interp1d([30,230],  [frame_height-50, frame_height-400])\n",
    "                    bar_value = bar_interp_f(distance)  \n",
    "                    \n",
    "                    # Write the current mode on the frame.\n",
    "                    cv2.putText(frame, f'{current_mode} {(distance-30)//2}%',\n",
    "                                (frame_width//2-(20*len(current_mode)), 40),\n",
    "                                cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 0), 3)\n",
    "                    \n",
    "                    # Draw the filled rectangle with varying height on the frame.\n",
    "                    cv2.rectangle(frame, (frame_width-80, int(bar_value)), \n",
    "                                  (frame_width-50, frame_height-50), (255, 0, 255), -1)\n",
    "                    \n",
    "                    # Draw another rectangle around the filled rectangle on the frame.\n",
    "                    cv2.rectangle(frame, (frame_width-80, frame_height-400),\n",
    "                                  (frame_width-50, frame_height-50), (0, 255, 0), 6)\n",
    "    # Otherwise.\n",
    "    else:\n",
    "        # Clear the buffer.\n",
    "        buffer.clear()\n",
    "        \n",
    "    # Check if the change brighness state is true.\n",
    "    if change_brightness:\n",
    "        \n",
    "        # Change the brighness of the frame. \n",
    "        frame = changeSatValue(frame, scale_factor=brightness_scale,\n",
    "                               channel='Value', display=False)\n",
    "    \n",
    "    # Check if the change saturation state is true.\n",
    "    if change_saturation:\n",
    "        \n",
    "        # Change the saturation of the frame. \n",
    "        frame = changeSatValue(frame, scale_factor=saturation_scale,\n",
    "                               channel='Saturation', display=False) \n",
    "    \n",
    "    # Check if the change contrast state is true.\n",
    "    if change_contrast:\n",
    "        \n",
    "        # Change the contrast of the frame. \n",
    "        frame = changeContrast(frame, contrast_scale, display=False)\n",
    "    \n",
    "    # Check if the change gamma state is true.\n",
    "    if change_gamma:\n",
    "        \n",
    "        # Perform gamma correction on the frame. \n",
    "        frame = gammaCorrection(frame, gamma_scale, display=False)\n",
    "    \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Live Gesture Control Application', frame)\n",
    "    \n",
    "    # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.\n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Additional comments:\n",
    "#       - In summary, this program counts the number of fingers\n",
    "#         that are up. Then, it checks which ones are up to \n",
    "#         determine what the gesture is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the final results are incredible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(255,140,0)\"> Code License Agreement </font>**\n",
    "```\n",
    "Copyright (c) 2022 Bleedai.com\n",
    "\n",
    "Feel free to use this code for your own projects commercial or noncommercial, these projects can be Research-based, just for fun, for-profit, or even Education with the exception that you’re not going to use it for developing a course, book, guide, or any other educational products.\n",
    "\n",
    "Under *NO CONDITION OR CIRCUMSTANCE* you may use this code for your own paid educational or self-promotional ventures without written consent from Taha Anwar (BleedAI.com).\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab2c83b243fb879010d169f2f59fe1d865a42357da3e2fb5ab94d633edfe058a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
