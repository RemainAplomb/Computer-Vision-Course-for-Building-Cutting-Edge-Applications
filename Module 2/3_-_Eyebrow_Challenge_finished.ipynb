{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07c6d1a-7f9e-40d2-ac08-82caee28c026",
   "metadata": {},
   "source": [
    "# **<center><font style=\"color:rgb(100,109,254)\">Module 2: Real-Time Controllable Face Makeup</font> </center>**\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1ERLn4WIEsSGnAO_4LDT8R8FTf6ac3f5n'>\n",
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\"> Module Outline </font>**\n",
    "\n",
    "The module can be split into the following parts:\n",
    "\n",
    "- *Lesson 1: Introduction to Face Landmark Detection Theory*\n",
    "\n",
    "- *Lesson 2: Create a Face Landmarks Detector*\n",
    "\n",
    "- ***Lesson 3:* Build a Face Part Selector** *(This Tutorial)*\n",
    "\n",
    "- *Lesson 4: Build a Virtual Face Makeup Application*\n",
    "\n",
    "- *Lesson 5: Build the Final Application*\n",
    "\n",
    "**Please Note**, these Jupyter Notebooks are not for sharing; do read the Copyright message below the Code License Agreement section which is in the last cell of this notebook.\n",
    "-Taha Anwar\n",
    "\n",
    "Alright, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6a004-b961-4b30-8d9c-81ba6ecac23f",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\"> Import the Libraries</font>**\n",
    "\n",
    "First, we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c2d391-9e95-4e07-bdd7-7d1d8b7321fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mediapipe version: 0.8.10.1, it should be 0.8.9.1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "from importlib.metadata import version\n",
    "from previous_lesson import detectFacialLandmarks, detectHandsLandmarks, countFingers\n",
    "print(f\"Mediapipe version: {version('mediapipe')}, it should be 0.8.9.1\")\n",
    "\n",
    "\n",
    "# Additional comments:\n",
    "#       - Use the previous functions on detection of hands, counting fingers\n",
    "#         and detection of facial features. The hands will be used as the controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557f87f-070f-4bde-8c33-31bb6a07aea7",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Initializations</font>**\n",
    "\n",
    "After that, in this step, we will perform all the initializations required to build the Face Parts Selector. Note that we want to select the Face Parts by touching them with our hands.\n",
    "\n",
    "### **<font style=\"color:rgb(134,19,348)\">Face Landmarks Detection Model</font>**\n",
    "\n",
    "So first, we will have to initialize the **`mp.solutions.face_mesh`** class and then set up the **`mp.solutions.face_mesh.FaceMesh()`** function with appropriate arguments, as we had done in the previous lesson. We will only be working with videos in this lesson, so we will only set up the **`mp.solutions.face_mesh.FaceMesh()`** function one time with **`static_image_mode`** to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40d1dc3-40f4-4d05-87bd-75cbd08c8b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe face mesh class.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Set up the face mesh function with appropriate arguments.\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True,\n",
    "                                  min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "\n",
    "# Additional comments:\n",
    "#       - Since we will not be detecting static images, we will\n",
    "#         only get an instance of the face mesh, and set the \n",
    "#         static image mode to False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eaa59d-ac1e-4443-be8f-b78c287d00a4",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Hands Landmarks Detection Model</font>**\n",
    "\n",
    "As mentioned we want to select the face parts with hands, so we will also need the **Mediapipe's Hands solution** (explained in detail in the previous module) as well, to get the exact location and gesture of the hands, so now we will have to initialize the **`mp.solutions.hands`** class and then set up the **`mp.solutions.hands.Hands()`** function with appropriate arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df922f4-4c74-4aa6-b397-8e15bd404e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe hands class.\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Set up the Hands function with appropriate arguments.\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2,\n",
    "                       min_detection_confidence=0.8, min_tracking_confidence=0.8)\n",
    "\n",
    "# Additional comments:\n",
    "#       - The hands mesh will be used to determine whether\n",
    "#         the tip of the pointing finger of the right hand\n",
    "#         is in the region of interest (e.g. Right cheeks, eyes, lips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d8224-3b54-426c-bd50-3204e0c20ff5",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Face Parts Indexes</font>**\n",
    "\n",
    "Now as we already know, for each face, **Mediapipe's Face Mesh solution** returns a list of **four hundred sixty-eight** facial landmarks that represent important regions of the face e.g, eyes, nose, lips, etc and to build a face parts selector we need to figure out a way to isolate the landmarks of the face parts from the face mesh returned by the solution. Well, luckily similar to the Mediapipe's Hand solution, the Face Mesh solution also always returns these landmarks in the same sequence meaning that for every face the landmark representing the nose tip will be at the index 1 in the list. You can find the indexes for the face parts, that you want to isolate in the image below. \n",
    "\n",
    "<center>\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1wOuAzZfXKJ1OZhE94PqGbv4ab1fPEZJy' width =\"600\"> \n",
    "    \n",
    "<br><a href=\"https://developers.google.com/ar/develop/augmented-faces\">Image Source</a>\n",
    "\n",
    "</center>\n",
    "    \n",
    "Now we will initialize a few lists containing the indexes of landmarks of the face parts (lips, right eye, and right cheek) that we want to select. Mediapipe also provides some frozenset objects (as attributes of the **`mp.solutions.face_mesh`** class) that contain indexes of some face parts landmark coordinates. \n",
    "\n",
    "* **`mp_face_mesh.FACEMESH_FACE_OVAL`** contains indexes of face outline.\n",
    "* **`mp_face_mesh.FACEMESH_LIPS`** contains indexes of lips.\n",
    "* **`mp_face_mesh.FACEMESH_LEFT_EYE`** contains indexes of left eye.\n",
    "* **`mp_face_mesh.FACEMESH_RIGHT_EYE`** contains indexes of right eye.\n",
    "* **`mp_face_mesh.FACEMESH_LEFT_EYEBROW`** contains indexes of left eyebrow.\n",
    "* **`mp_face_mesh.FACEMESH_RIGHT_EYEBROW`** contains indexes of right eyebrow.\n",
    "\n",
    "But these frozenset objects contain the indexes of all the landmarks of the face parts and we only need the landmarks indexes of the outlines of these face parts so that's why we are going with manually defining the indexes lists, this approach no doubt is more time consuming as we have to manually find the indexes but gives much more control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0243eda4-e445-40a7-8cc3-b127c81d23af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the indexes of the upper lips outer outline landmarks.\n",
    "lips_upper_outer_ids = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]\n",
    "\n",
    "# Initialize a list to store the indexes of the lower lips outer outline landmarks.\n",
    "lips_lower_outer_ids = [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291]\n",
    "\n",
    "# Initialize a list to store the indexes of the upper part of the right eye outline landmarks.\n",
    "right_eye_upper_ids = [342, 445, 444, 443, 442, 441, 413]\n",
    "\n",
    "# Initialize a list to store the indexes of the lower part of the right eye outline landmarks.\n",
    "right_eye_lower_ids = [463, 341, 256, 252, 253, 254, 339, 255, 359]\n",
    "\n",
    "# Initialize a list to store the indexes of the right cheek outline landmarks.\n",
    "right_cheek_landmarks_ids = [379, 365, 397, 288, 361, 323, 454, 356, 372, 346, 280, 425, 432, 430]\n",
    "\n",
    "\n",
    "# For homework, eyebrow landmarks\n",
    "# Initialize a list to store the indexes of the upper part of the right eye outline landmarks.\n",
    "# With this upper ids, the eyebrow fits perfectly.\n",
    "# However, when i'm pointing to the eyebrow, it doesn't detect\n",
    "#right_eyebrow_upper_ids = [293, 334, 296, 336]\n",
    "\n",
    "right_eyebrow_upper_ids = [293, 333, 299, 337, 336]\n",
    "right_eyebrow_lower_ids = [285, 295, 282, 283, 276]\n",
    "\n",
    "left_eyebrow_upper_ids = [63, 104, 69, 108, 151, 9, 8]\n",
    "left_eyebrow_lower_ids = [55, 65, 52, 53, 46]\n",
    "\n",
    "# Additional comments:\n",
    "#       - This will contain the index of the landmarks\n",
    "#         that is in our targetted region of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb9db9-c259-4279-b876-90a4b8608d97",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Create a Face Part Selection Function</font>**\n",
    "\n",
    "Now we will create a function **`selectFacePart()`** that will allow the user to select **Face, Eyes, and Lips** by just touching these face parts in real-time with the index fingertip while making the `INDEX POINTING UP` hand gesture (☝️). We will utilize the **`countFingers()`** function from the previous module, to count the fingers up and get the tips landmarks, and then we will use the OpenCV's [**`cv2.pointPolygonTest()`**](https://docs.opencv.org/3.4/d3/dc0/group__imgproc__shape.html#ga1a539e8db2135af2566103705d7a5722) function to check whether the finger is touching a face part. We will also use the [**`cv2.drawContours()`**](https://docs.opencv.org/3.4/d6/d6e/group__imgproc__draw.html#ga746c0625f1781f1ffc9056259103edbc) function to highlight the selectable face parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83378269-f2e0-47ce-ad03-0d1e787d6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectFacePart(image, face_landmarks, hands_results, hand_label='RIGHT'):\n",
    "    '''\n",
    "    This function will allow the user to select face parts utilizing hand gestures.\n",
    "    Args:\n",
    "        image:          The image/frame of the user with his index finger pointing towards a face part to select.\n",
    "        face_landmarks: An array containing the face landmarks (x and y coordinates) of the face in the image.\n",
    "        hands_results:  The output of the hands landmarks detection performed on the image. \n",
    "        hand_label:     The label of the hand i.e. left or right, of which the gesture is required to be recognized. \n",
    "    Returns:\n",
    "        output_image:       A copy of the input image with transparent contours drawn, highlighting the selectable face parts. \n",
    "        selected_face_part: The name of the face part selected by the user in the image/frame.\n",
    "    '''\n",
    "    \n",
    "    # Initialize a variable to store the selected face part.\n",
    "    selected_face_part = None\n",
    "    \n",
    "    # Create a copy of the input image.\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    # Initialize a list to store the lips landmarks.\n",
    "    lips_landmarks = []\n",
    "    \n",
    "    # Initialize a list to store the right eye landmarks.\n",
    "    right_eye_landmarks = []\n",
    "\n",
    "    # For homework, eyebrow landmarks\n",
    "    right_eyebrow_landmarks = []\n",
    "    left_eyebrow_landmarks = []\n",
    "    \n",
    "    # Initialize a list to store the right cheek landmarks.\n",
    "    right_cheek_landmarks = []\n",
    "    \n",
    "    # Get the height and width of the image.\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Get the count of fingers up, fingers statuses, and tips landmarks of the detected hand(s).\n",
    "    # I have modified this countFingers() function from previous module to ignore the thumbs count logic, if consider_thumbs\n",
    "    # is False, to remove the limatation of always having to face the palm of hand towards the camera to get correct results.\n",
    "    count, fingers_statuses, fingers_tips_position = countFingers(image, hands_results, consider_thumbs=False,\n",
    "                                                                  draw=False, display=False)\n",
    "    \n",
    "    # Check if the number of the fingers up of the selector hand is 1 and the finger that is up, is the index finger.\n",
    "    # And the number of the fingers up, of the opposite hand is 0.\n",
    "    if count[hand_label] == 1 and fingers_statuses[hand_label+'_INDEX'] \\\n",
    "    and count['LEFT' if hand_label=='RIGHT' else 'RIGHT'] == 0:\n",
    "        \n",
    "        # Get the x and y coordinates of the tip landmark of the index finger of the selector hand. \n",
    "        index_x, index_y = fingers_tips_position[hand_label]['INDEX']\n",
    "        \n",
    "        # Lips Selection part.\n",
    "        ####################################################################################################################\n",
    "        \n",
    "        # Iterate over the indexes of the upper and lower lips outline.\n",
    "        for index in lips_upper_outer_ids+lips_lower_outer_ids:\n",
    "            \n",
    "            # Get the landmark at the index we are iterating upon,\n",
    "            # And append it into the list.\n",
    "            lips_landmarks.append(face_landmarks[index])\n",
    "        \n",
    "        # Convert the lips landmarks list into a numpy array.\n",
    "        lips_landmarks = np.array(lips_landmarks, np.int32)\n",
    "        \n",
    "        # Draw filled lips contours on the copy of the image.\n",
    "        cv2.drawContours(output_image, contours=[lips_landmarks], contourIdx=-1, \n",
    "                         color=(255, 255, 255), thickness=-1)\n",
    "        \n",
    "        # Check if the index finger tip is inside the lips contours (outline). \n",
    "        if cv2.pointPolygonTest(lips_landmarks,(index_x, index_y), measureDist=False)  == 1:\n",
    "            \n",
    "            # Update the selected face part variable to LIPS.\n",
    "            selected_face_part = 'LIPS'\n",
    "            \n",
    "        # Eyes Selection part.\n",
    "        ####################################################################################################################\n",
    "        \n",
    "        # Iterate over the indexes of the right eye ouline.\n",
    "        for index in right_eye_upper_ids+right_eye_lower_ids:\n",
    "            \n",
    "            # Get the landmark at the index we are iterating upon,\n",
    "            # And append it into the list.\n",
    "            right_eye_landmarks.append(face_landmarks[index])\n",
    "        \n",
    "        # Convert the right eye landmarks list into a numpy array.\n",
    "        right_eye_landmarks = np.array(right_eye_landmarks, np.int32)\n",
    "        \n",
    "        # Draw filled right eye contours on the copy of the image.\n",
    "        cv2.drawContours(output_image, contours=[right_eye_landmarks], contourIdx=-1, \n",
    "                         color=(255, 255, 255), thickness=-1)  \n",
    "        \n",
    "        # Check if the index finger tip is inside the right eye contours (outline). \n",
    "        if cv2.pointPolygonTest(right_eye_landmarks,(index_x, index_y), measureDist=False)  == 1:\n",
    "            \n",
    "            # Update the selected face part variable to EYES.\n",
    "            selected_face_part = 'EYES'\n",
    "        \n",
    "        # Face Selection part.\n",
    "        ####################################################################################################################\n",
    "        \n",
    "        # Iterate over the indexes of the right cheek ouline.\n",
    "        for index in right_cheek_landmarks_ids:\n",
    "            \n",
    "            # Get the landmark at the index we are iterating upon,\n",
    "            # And append it into the list.\n",
    "            right_cheek_landmarks.append(face_landmarks[index])\n",
    "        \n",
    "        # Convert the right cheek landmarks list into a numpy array.\n",
    "        right_cheek_landmarks = np.array(right_cheek_landmarks, np.int32)\n",
    "        \n",
    "        # Draw filled right cheek contours on the copy of the image.\n",
    "        cv2.drawContours(output_image, contours=[right_cheek_landmarks], contourIdx=-1, \n",
    "                         color=(255, 255, 255), thickness=-1)    \n",
    "        \n",
    "        # Check if the index finger tip is inside the right cheek contours (outline). \n",
    "        if cv2.pointPolygonTest(right_cheek_landmarks,(index_x, index_y), measureDist=False)  == 1:\n",
    "            \n",
    "            # Update the selected face part variable to FACE.\n",
    "            selected_face_part = 'FACE'\n",
    "        \n",
    "\n",
    "        # Eyebrow Selection Part (Challenge)\n",
    "        ####################################################################################################################\n",
    "        # Iterate over the indexes of the eyebrow ouline.\n",
    "\n",
    "        # Right Eyebrow\n",
    "        for index in right_eyebrow_upper_ids+right_eyebrow_lower_ids:\n",
    "            \n",
    "            # Get the landmark at the index we are iterating upon,\n",
    "            # And append it into the list.\n",
    "            right_eyebrow_landmarks.append(face_landmarks[index])\n",
    "        \n",
    "        # Convert the right cheek landmarks list into a numpy array.\n",
    "        right_eyebrow_landmarks = np.array(right_eyebrow_landmarks, np.int32)\n",
    "        \n",
    "        # Draw filled right cheek contours on the copy of the image.\n",
    "        cv2.drawContours(output_image, contours=[right_eyebrow_landmarks], contourIdx=-1, \n",
    "                         color=(255, 255, 255), thickness=-1)    \n",
    "        \n",
    "        # Check if the index finger tip is inside the right cheek contours (outline). \n",
    "        if cv2.pointPolygonTest(right_eyebrow_landmarks,(index_x, index_y), measureDist=False)  == 1:\n",
    "            \n",
    "            # Update the selected face part variable to Right Eyebrow.\n",
    "            selected_face_part = 'RIGHT EYEBROW'\n",
    "\n",
    "        \n",
    "        # Left Eyebrow\n",
    "        for index in left_eyebrow_upper_ids+left_eyebrow_lower_ids:\n",
    "            \n",
    "            # Get the landmark at the index we are iterating upon,\n",
    "            # And append it into the list.\n",
    "            left_eyebrow_landmarks.append(face_landmarks[index])\n",
    "        \n",
    "        # Convert the left eyebrow landmarks list into a numpy array.\n",
    "        left_eyebrow_landmarks = np.array(left_eyebrow_landmarks, np.int32)\n",
    "        \n",
    "        # Draw filled right cheek contours on the copy of the image.\n",
    "        cv2.drawContours(output_image, contours=[left_eyebrow_landmarks], contourIdx=-1, \n",
    "                         color=(255, 255, 255), thickness=-1)    \n",
    "        \n",
    "        # Check if the index finger tip is inside the right cheek contours (outline). \n",
    "        if cv2.pointPolygonTest(left_eyebrow_landmarks,(index_x, index_y), measureDist=False)  == 1:\n",
    "            \n",
    "            # Update the selected face part variable to Left Eyebrow.\n",
    "            selected_face_part = 'LEFT EYEBROW'\n",
    "        \n",
    "        ####################################################################################################################\n",
    "\n",
    "\n",
    "        # Perform weighted addition between the original image and \n",
    "        # its copy with the contours drawn to get a transparency effect. \n",
    "        output_image = cv2.addWeighted(output_image, 0.35, image, 0.65, 0)\n",
    "    \n",
    "    # Return the image with transparent contours drawn, and the selected face part\n",
    "    return output_image, selected_face_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef8e71-bd4f-444e-ad9f-cca398df80a5",
   "metadata": {},
   "source": [
    "Now lets see how this **`selectFacePart()`** function works on a real-time webcam feed. To remove the false positives we will only consider a face part selected when the **`selectFacePart()`** function returns that face part name multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce0573fd-09c2-47d2-b473-a1fde2129298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# Removed these\n",
    "#camera_video.set(3,1280)\n",
    "#camera_video.set(4,960)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Face Part Selection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Create a buffer to store the selected face part.\n",
    "selected_face_part = deque([], maxlen=20)\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then \n",
    "    # continue to the next iteration to read the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "    \n",
    "    # Get the height and width of the frame of the webcam video. \n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Perform Face landmarks detection.\n",
    "    frame, face_landmarks = detectFacialLandmarks(frame, face_mesh, draw=False, display=False)\n",
    "    \n",
    "    # Check if the Face landmarks in the frame are detected.\n",
    "    if len(face_landmarks)>0:\n",
    "        \n",
    "        # Perform Hands landmarks detection on the frame.\n",
    "        frame, hands_results = detectHandsLandmarks(frame, hands, draw=True, display=False)\n",
    "        \n",
    "         # Check if the hands landmarks in the frame are detected.\n",
    "        if hands_results.multi_hand_landmarks:\n",
    "            \n",
    "            # Perform the Face part selection process utilizing the face and hands landmarks.\n",
    "            frame, currently_selected_part = selectFacePart(frame, face_landmarks, hands_results)\n",
    "            \n",
    "            # Append the Face part selection results into the buffer.\n",
    "            selected_face_part.append(currently_selected_part)\n",
    "            \n",
    "            # Check if the length of the buffer is equal to 20 i.e. the max length.\n",
    "            if len(selected_face_part) == 20:\n",
    "                \n",
    "                # Check if the current maximum face part selection results in the buffer are LIPS.\n",
    "                if max(set(selected_face_part), key=selected_face_part.count) == 'LIPS':\n",
    "                    \n",
    "                    # Write LIPS Selected text on the frame. \n",
    "                    cv2.putText(frame, 'LIPS Selected.', (5, int(frame_height-20)),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
    "                    \n",
    "                # Check if the current maximum face part selection results in the buffer are FACE.\n",
    "                elif max(set(selected_face_part), key=selected_face_part.count) == 'FACE':\n",
    "                    \n",
    "                    # Write FACE Selected text on the frame. \n",
    "                    cv2.putText(frame, 'FACE Selected.', (5, int(frame_height-20)),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
    "                \n",
    "                # Check if the current maximum face part selection results in the buffer are EYES.\n",
    "                elif max(set(selected_face_part), key=selected_face_part.count) == 'EYES':\n",
    "                    \n",
    "                    # Write EYES Selected text on the frame. \n",
    "                    cv2.putText(frame, 'EYES Selected.', (5, int(frame_height-20)),\n",
    "                                 cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
    "\n",
    "                # Check if the current maximum face part selection results in the buffer are RIGHT EYEBROWS\n",
    "                elif max(set(selected_face_part), key=selected_face_part.count) == 'RIGHT EYEBROW':\n",
    "                    # Write EYES Selected text on the frame. \n",
    "                    cv2.putText(frame, 'RIGHT EYEBROW.', (5, int(frame_height-20)),\n",
    "                                 cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
    "                \n",
    "                # Check if the current maximum face part selection results in the buffer are RIGHT EYEBROWS\n",
    "                elif max(set(selected_face_part), key=selected_face_part.count) == 'LEFT EYEBROW':\n",
    "                    # Write EYES Selected text on the frame. \n",
    "                    cv2.putText(frame, 'LEFT EYEBROW.', (5, int(frame_height-20)),\n",
    "                                 cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
    "                    \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Face Part Selection', frame)\n",
    "    \n",
    "    # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF    \n",
    "    \n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.                  \n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a836f-8dc9-47b2-a954-cc1dbfa391fc",
   "metadata": {},
   "source": [
    "Nice! working perfectly fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f6eba-08c1-4f6f-ae26-14324cad9d12",
   "metadata": {},
   "source": [
    "##  <font style=\"color:rgb(34,169,134)\">Challenge (Optional)</font>\n",
    "\n",
    "Inside the **`selectFacePart()`** function created above, try to add the functionality of selecting individual eyebrows, this can be used in the next lesson, where you'll be change the eyebrow colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0cc8a00-8f63-40f1-8d09-d05b90fabf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE\n",
    "\n",
    "# I finished the challenge here, I directly added it to the select face part function.\n",
    "# I have also recorded myself testing it\n",
    "\n",
    "# As for my references on the landmarks, here it is:\n",
    "#       https://github.com/ManuelTS/augmentedFaceMeshIndices/blob/master/Frong_wireframe.jpg\n",
    "#       https://github.com/ManuelTS/augmentedFaceMeshIndices/blob/master/Right_Eye.jpg\n",
    "#       https://github.com/ManuelTS/augmentedFaceMeshIndices/blob/master/Left_Eye.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e71b01-8f06-487f-9f4b-c9e265df8672",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(255,140,0)\"> Code License Agreement </font>**\n",
    "```\n",
    "Copyright (c) 2022 Bleedai.com\n",
    "\n",
    "Feel free to use this code for your own projects commercial or noncommercial, these projects can be Research-based, just for fun, for-profit, or even Education with the exception that you’re not going to use it for developing a course, book, guide, or any other educational products.\n",
    "\n",
    "Under *NO CONDITION OR CIRCUMSTANCE* you may use this code for your own paid educational or self-promotional ventures without written consent from Taha Anwar (BleedAI.com).\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab2c83b243fb879010d169f2f59fe1d865a42357da3e2fb5ab94d633edfe058a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
