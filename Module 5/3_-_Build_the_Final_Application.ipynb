{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<center><font style=\"color:rgb(100,109,254)\">Module 5: Real-Time Style Transfer Painting Portrait</font> </center>**\n",
    "\n",
    "<center>\n",
    "    <img src='https://drive.google.com/uc?export=download&id=1RENayQoiXrIIpuheGqM3G6UOAqVfEj9b'> </center>\n",
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\"> Module Outline </font>**\n",
    "\n",
    "The module can be split into the following parts:\n",
    "\n",
    "- *Lesson 1: Introduction to Neural Style Transfer Theory*\n",
    "\n",
    "- *Lesson 2: Apply Neural Style Transfer with OpenCV*\n",
    "\n",
    "- ***Lesson 3:* Build the Final Application** *(This Tutorial)*\n",
    "\n",
    "**Please Note**, these Jupyter Notebooks are not for sharing; do read the Copyright message below the Code License Agreement section which is in the last cell of this notebook.\n",
    "-Taha Anwar\n",
    "\n",
    "Alright, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\"> Import the Libraries</font>**\n",
    "\n",
    "First, we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from time import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "from previous_lesson import (detectHandsLandmarks, applyNeuralStyleTransfer, \n",
    "                             recognizeGestures, calculateDistance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Initialize the Hands Landmarks Detection Model</font>**\n",
    "\n",
    "After that, we will have to initialize the **`mp.solutions.hands`** class and then set up the **`mp.solutions.hands.Hands()`** function with appropriate arguments, as we have been doing in the previous modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe hands class.\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Set up the Hands functions for videos.\n",
    "hands_videos = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Create a Function to apply Neural Style Transfer</font>**\n",
    "\n",
    "Now we will create a function **`paint()`**, that will simply utilize the hands' landmarks (found by the **`detectHandsLandmarks()`** function, created in a previous module) to calculate the size and location (i.e. at the center of the hands) of a paintbrush (shaped like a flower) and then paint on a `canvas` *(i.e. just an empty black image)*, that will be utilized later to apply Neural Style Transfer only on the painted part of a webcam feed in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paint(frame, canvas, results, current_mode, hands_tips_positions):\n",
    "    '''\n",
    "    This function will paint with a very creative shape brush (like a flower) on a canvas.\n",
    "    Args:\n",
    "        frame:                A frame/image from the webcam feed.\n",
    "        canvas:               A black image equal to the webcam feed size, to paint on.\n",
    "        current_mode:         The current mode enabled i.e., either paint or erase.\n",
    "        hands_tips_positions: A dictionary containing the landmarks of the tips of the fingers of the hands.\n",
    "    Returns:    \n",
    "        frame:  The frame with the eraser drawn on it, in case the erase mode was enabled.\n",
    "        canvas: The black image with the intented drawings on it.\n",
    "    '''\n",
    "    \n",
    "    # Get the height and width of the frame.\n",
    "    height, width, _ = frame.shape\n",
    "    \n",
    "    # Iterate over the found hands in the frame.\n",
    "    for hand_index, hand_info in enumerate(results.multi_handedness):\n",
    "        \n",
    "        # Retrieve the label of the found hand.\n",
    "        hand_label = hand_info.classification[0].label.upper()\n",
    "        \n",
    "        # Retrieve the landmarks of the found hand.\n",
    "        hand_landmarks =  results.multi_hand_landmarks[hand_index]\n",
    "        \n",
    "        # Get the wrist landmarks of the hand, we are iterating upon.\n",
    "        x1, y1 = (int(hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].x*width),\n",
    "                  int(hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].y*height))\n",
    "        \n",
    "        # Get the middle finger tip landmarks.\n",
    "        x2, y2 = hands_tips_positions[hand_label]['MIDDLE']\n",
    "        \n",
    "        # Calculate the center of the hand, we are iterating upon.\n",
    "        hand_center_x = (x1+x2)//2\n",
    "        hand_center_y = (y1+y2)//2\n",
    "        \n",
    "        # Calculate the brush size accordingly to the hand size, we are iterating upon.\n",
    "        brush_size = calculateDistance(frame, point1=hands_tips_positions[hand_label]['PINKY'],\n",
    "                                       point2=hands_tips_positions[hand_label]['THUMB'], draw=False, display=False)\n",
    "        \n",
    "        # Calculate the radius of the brush.\n",
    "        radius = brush_size//2\n",
    "        \n",
    "        # Initialize a list to store the brush (of the hand, we are iterating upon) coordinates.\n",
    "        brush_pts = []\n",
    "        \n",
    "        # Iterate over 20 times.\n",
    "        # As the brush will have 20 corners and will be shaped like a flower.\n",
    "        for i in range(20):\n",
    "            \n",
    "            # Get the x and y coordinates of a brush corner.\n",
    "            x = hand_center_x + radius* math.cos(i * 18 * math.pi / 20)\n",
    "            y = hand_center_y + radius * math.sin(i * 18 * math.pi / 20) \n",
    "            \n",
    "            # Append the coordinates into the list.\n",
    "            brush_pts.append((x, y))\n",
    "        \n",
    "        # Check if the paint mode is enabled.\n",
    "        if current_mode == 'Paint Mode':\n",
    "            \n",
    "            # Draw the shape (brush) with white color, on the canvas.\n",
    "            canvas = cv2.fillPoly(canvas, pts = [np.array(brush_pts, np.int32)],\n",
    "                                  color=(255,255,255))\n",
    "        \n",
    "        # Check if the erase mode is enabled.\n",
    "        elif current_mode == 'Erase Mode':\n",
    "            \n",
    "            # Draw the shape (brush) with black color, on the canvas.\n",
    "            # This will erase the paint at the brush location.\n",
    "            canvas = cv2.fillPoly(canvas, pts = [np.array(brush_pts, np.int32)],\n",
    "                                  color=(0,0,0))\n",
    "            \n",
    "            # Create a copy of the frame.\n",
    "            frame_copy = frame.copy()\n",
    "            \n",
    "            # Draw the shape (brush) with white color, on the copy of the frame.\n",
    "            # This is drawn just to represent an eraser on the hands locations.\n",
    "            frame_copy = cv2.fillPoly(frame_copy, pts = [np.array(brush_pts, np.int32)],\n",
    "                                color=(255,255,255))\n",
    "            \n",
    "            # Perform weighted addition to give the eraser a transparency look.\n",
    "            frame = cv2.addWeighted(frame, 0.5, frame_copy, 0.5, 0)\n",
    "    \n",
    "    # Return the frame and the canvas.\n",
    "    return frame, canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will utilize the function **`paint()`** and the `canvas` (it created) to apply neural style transfer only on the intended parts of a webcam feed in real-time. We will be switching between `PAINT` and `ERASE` mode with the hand gestures (`INDEX POINTING UP` to enable paint mode and `VICTORY` to enable erase mode) and with the keys `P` (for paint) and `E` (for erase) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Initialize a canvas to draw on.\n",
    "canvas = np.zeros(shape=(int(camera_video.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
    "                         int(camera_video.get(cv2.CAP_PROP_FRAME_WIDTH)), 3),\n",
    "                  dtype=np.uint8)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Real-Time Style Transfer Painting Portrait', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Specify the path of the directory which contains the trained models.\n",
    "model_folder = 'models'\n",
    "\n",
    "# Initialize a list to store the loaded models.\n",
    "models = []\n",
    "\n",
    "# Iterate over the models files in the directory.\n",
    "for model_name in os.listdir(model_folder):\n",
    "    \n",
    "    # Load a model and append it into the list.\n",
    "    models.append(cv2.dnn.readNetFromTorch(os.path.join(model_folder,model_name)))\n",
    "\n",
    "# Initialize a variable to store the index of the selected model.\n",
    "selected_model_index=0\n",
    "\n",
    "# Initialize a variable to store the buffer length.\n",
    "BUFFER_MAX_LENGTH = 2\n",
    "\n",
    "# Initialize a buffer to store recognized gestures.\n",
    "buffer = deque([], maxlen=BUFFER_MAX_LENGTH)\n",
    "\n",
    "# Initialize a variable to store the current mode.\n",
    "current_mode = None\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "   \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if the frame is not read properly,\n",
    "    # then continue to the next iteration to read the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the height and width of the frame of the webcam video.\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    \n",
    "    # Apply neural style transfer on the frame using the selected model.\n",
    "    nst_frame = applyNeuralStyleTransfer(frame, models[selected_model_index % len(models)], display=False)\n",
    "    \n",
    "    # Perform Hands landmarks detection on the frame.\n",
    "    frame, results = detectHandsLandmarks(frame, hands_videos, draw=False, display=False)\n",
    "    \n",
    "    # Check if the hands landmarks in the frame are detected.\n",
    "    if results.multi_hand_landmarks:  \n",
    "        \n",
    "        # Perform a hand gesture recognition.\n",
    "        current_gesture, hands_tips_positions = recognizeGestures(frame, results, 'RIGHT', draw=False,\n",
    "                                                                  display=False)\n",
    "        # Check if a known gesture is recognized.\n",
    "        if current_gesture != 'UNKNOWN':\n",
    "            \n",
    "            # Check if all the gestures stored in the buffer are equal to the current gesture.\n",
    "            if all(current_gesture==gesture for gesture in buffer):\n",
    "                \n",
    "                # Append the current gesture into the buffer.\n",
    "                buffer.append(current_gesture)\n",
    "                \n",
    "            # Otherwise.\n",
    "            else:\n",
    "                \n",
    "                # Clear the buffer.\n",
    "                buffer.clear()\n",
    "            \n",
    "            # Check if the length of the buffer is equal to the maxlength, that is 20.\n",
    "            if len(buffer) == BUFFER_MAX_LENGTH:\n",
    "                \n",
    "                # Check if the current hand gesture is INDEX POINTING UP.\n",
    "                if current_gesture == 'INDEX POINTING UP':\n",
    "                    \n",
    "                    # Enable the paint mode.\n",
    "                    current_mode = 'Paint Mode'\n",
    "                    \n",
    "                # Check if the current hand gesture is VICTORY.\n",
    "                elif current_gesture == 'VICTORY':\n",
    "                    \n",
    "                    # Enable the erase mode.\n",
    "                    current_mode = 'Erase Mode'\n",
    "        \n",
    "        # Paint or Erase on the canvas depending upon the current mode enabled. \n",
    "        frame, canvas = paint(frame, canvas, results, current_mode, hands_tips_positions)\n",
    "    \n",
    "    # Update the pixel values of the frame with the nst frame (neural style transfer applied) \n",
    "    # values at the indexes where canvas!=0 that is where something is drawn on the canvas.\n",
    "    #frame[np.mean(canvas, axis=2)!=0] = nst_frame[np.mean(canvas, axis=2)!=0]\n",
    "    frame[np.mean(canvas, axis=2)!=0] = nst_frame[np.mean(canvas, axis=2)!=0]\n",
    "\n",
    "    \n",
    "    # Check if a mode is enabled.\n",
    "    if current_mode:\n",
    "        \n",
    "        # Write the enabled mode on the frame.\n",
    "        cv2.putText(frame, current_mode + ' Enabled', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                    (0, 255, 0), 1)\n",
    "    \n",
    "    # Write the instructions (to switch modes and neural style) on the frame.\n",
    "    cv2.putText(frame, 'Press P to Paint', (5, frame_height-10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    cv2.putText(frame, 'Press E to Erase', (5, frame_height-30), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.5, (255, 255, 255), 1)\n",
    "    \n",
    "    cv2.putText(frame, 'Press S to Switch Neural Style', (5, frame_height-50),  cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.5,(255, 255, 255), 1)\n",
    "    \n",
    "    # Display the frame.\n",
    "    cv2.imshow(\"canvas\", canvas)\n",
    "    cv2.imshow(\"Real-Time Style Transfer Painting Portrait\", frame)\n",
    "    cv2.imshow(\"nst\", nst_frame/255)\n",
    "\n",
    "    \n",
    "    # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "     # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "        \n",
    "    # Check if 's' is pressed then increment the neural style transfer model index.\n",
    "    elif (k == ord('s')):\n",
    "        selected_model_index = selected_model_index + 1 \n",
    "    \n",
    "    # Check if 'p' is pressed then enable the paint mode.\n",
    "    elif (k == ord('p')):\n",
    "        current_mode = 'Paint Mode'\n",
    "    \n",
    "    # Check if 'e' is pressed then enable the erase mode.\n",
    "    elif (k == ord('e')):\n",
    "        current_mode = 'Erase Mode'\n",
    "    \n",
    "    # Check if 'c' is pressed then clear the canvas.\n",
    "    elif k == ord('c'):\n",
    "        canvas = np.zeros(shape=(frame_height, frame_width, 3), dtype=np.uint8)\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.\n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! the application is working as we intended. We can apply Neural Style Transfer anywhere we want with just the movements of our hands in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(255,140,0)\"> Code License Agreement </font>**\n",
    "```\n",
    "Copyright (c) 2022 Bleedai.com\n",
    "\n",
    "Feel free to use this code for your own projects commercial or noncommercial, these projects can be Research-based, just for fun, for-profit, or even Education with the exception that you’re not going to use it for developing a course, book, guide, or any other educational products.\n",
    "\n",
    "Under *NO CONDITION OR CIRCUMSTANCE* you may use this code for your own paid educational or self-promotional ventures without written consent from Taha Anwar (BleedAI.com).\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab2c83b243fb879010d169f2f59fe1d865a42357da3e2fb5ab94d633edfe058a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
