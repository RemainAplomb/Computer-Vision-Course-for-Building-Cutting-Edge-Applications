{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce6d6f3-9657-4cb3-baf7-c9ca9a87000c",
   "metadata": {},
   "source": [
    "# **<center><font style=\"color:rgb(100,109,254)\">Module 8: Emotion Recognition + AI Filters</font> </center>**\n",
    "\n",
    "<center>\n",
    "    <img src='https://drive.google.com/uc?export=download&id=1ekabh-KWOZhj8UPjf5AbZLzQ767z52_T' width=800> \n",
    "    <br/>\n",
    "    <a href='https://www.shutterstock.com/image-photo/emotion-detected-by-artificial-intelligence-ai-1898196328'>Image Credits</a>\n",
    "</center>\n",
    "    \n",
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\"> Module Outline </font>**\n",
    "\n",
    "The module can be split into the following parts:\n",
    "\n",
    "- *Lesson 1: Introduction to Support Vector Machine Theory.*\n",
    "\n",
    "- *Lesson 2: Train an Emotion Recognition SVM on FER Dataset.*\n",
    "\n",
    "- ***Lesson 3:* Create your own Dataset for Emotion Recognition.** *(This Tutorial)*\n",
    "\n",
    "- *Lesson 4: Create AI Filters With Emotion Recognition Based Triggers.*\n",
    "\n",
    "\n",
    "**Please Note**, these Jupyter Notebooks are not for sharing; do read the Copyright message below the Code License Agreement section which is in the last cell of this notebook.\n",
    "-Taha Anwar\n",
    "\n",
    "Alright, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1dc82-cb28-42bd-a487-94c924274b6e",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\"> Import the Libraries</font>**\n",
    "\n",
    "First, we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09836897-d496-4988-8fcb-5a9193823eef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mediapipe version: 0.8.10.1, it should be 0.8.9.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from previous_lesson import detectFacialLandmarks, predictEmotion\n",
    "from importlib.metadata import version\n",
    "print(f\"Mediapipe version: {version('mediapipe')}, it should be 0.8.9.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e245fc9-9262-4362-8db7-95cc9d2db5e4",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Initialize the Face Landmarks Detection Model</font>**\n",
    "\n",
    "After that, as we had done in the previous lesson, we will initialize the **`mp.solutions.face_mesh`** class and set up the **`mp.solutions.face_mesh.FaceMesh()`** function (for images and videos as well) with appropriate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "418df1f5-4845-45a4-a145-91ee59d3255c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the mediapipe face mesh class.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Setup the face landmarks function for images.\n",
    "face_mesh_images = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1,\n",
    "                                         refine_landmarks=True, min_detection_confidence=0.3)\n",
    "\n",
    "# Setup the face landmarks function for videos.\n",
    "face_mesh_videos = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1,\n",
    "                                         refine_landmarks=True, min_detection_confidence=0.8, \n",
    "                                         min_tracking_confidence=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab4825d-453a-47a6-aceb-f006a3c75b64",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Create a Function to Calculate Size of a Face Part</font>**\n",
    "\n",
    "In this lesson, instead of just passing all the `468` Face landmarks to the model (and letting the model figure out the pattern all by itself), we will try to remove the extra landmarks (like nose landmarks don't add much value to differentiate expressions) and extract some meaningful info (like size of mouth, and eyes) from the landmarks beforehand passing them into the model for training and for this purpose now we will create a function **`getSize()`** that will utilize detected landmarks to calculate the size of a face part. To isolate the landmarks of a face part we will use the frozenset objects (attributes of the **`mp.solutions.face_mesh`** class), which contain the required indexes.\n",
    "\n",
    "- **`mp_face_mesh.FACEMESH_FACE_OVAL`** contains indexes of face outline.\n",
    "- **`mp_face_mesh.FACEMESH_LIPS`** contains indexes of lips.\n",
    "- **`mp_face_mesh.FACEMESH_LEFT_EYE`** contains indexes of left eye.\n",
    "- **`mp_face_mesh.FACEMESH_RIGHT_EYE`** contains indexes of right eye.\n",
    "- **`mp_face_mesh.FACEMESH_LEFT_EYEBROW`** contains indexes of left eyebrow.\n",
    "- **`mp_face_mesh.FACEMESH_RIGHT_EYEBROW`** contains indexes of right eyebrow.\n",
    "\n",
    "After retrieving the landmarks of the face part, we will simply pass it to the function [**`cv2.boundingRect()`**](https://docs.opencv.org/4.5.3/d3/dc0/group__imgproc__shape.html#ga103fcbda2f540f3ef1c042d6a9b35ac7) to get the width and height of the face part. The function **`cv2.boundingRect(landmarks)`** returns the coordinates **(`x1`, `y1`, `width`, `height`)** of a bounding box enclosing the object (face part), given the landmarks but we will only need the **`height`** and **`width`** of the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb070e3-79f9-4391-8ca0-068d15ae69cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getSize(image, face_landmarks, INDEXES):\n",
    "    '''\n",
    "    This function calculates the height and width of a face part utilizing its landmarks.\n",
    "    Args:\n",
    "        image:          The image of the person whose face part size is to be calculated.\n",
    "        face_landmarks: The detected face landmarks of the person whose face part size is to \n",
    "                        be calculated.\n",
    "        INDEXES:        The indexes of the face part landmarks, whose size is to be calculated.\n",
    "    Returns:\n",
    "        width:                The calculated width of the face part of the face whose landmarks indexes were passed.\n",
    "        height:               The calculated height of the face part of the face whose landmarks indexes were passed.\n",
    "        normalized_landmarks: A list containing the normalized landmarks of the face part whose size is calculated.\n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    # Convert the indexes of the landmarks of the face part into a list.\n",
    "    # Also convert it into a set, to remove the duplicate indexes.\n",
    "    INDEXES_LIST = set(list(itertools.chain(*INDEXES)))\n",
    "    \n",
    "    # Initialize a list to store the landmarks of the face part.\n",
    "    landmarks = []\n",
    "    \n",
    "    # Initialize a list to store the normalized landmarks of the face part.\n",
    "    normalized_landmarks = []\n",
    "        \n",
    "    # Iterate over the indexes of the landmarks of the face part. \n",
    "    for INDEX in INDEXES_LIST:\n",
    "        \n",
    "        # Append the landmark into the list.\n",
    "        landmarks.append(face_landmarks[INDEX])\n",
    "        \n",
    "        # Normalize the landmark and append it into the list.\n",
    "        normalized_landmarks.append((face_landmarks[INDEX][0]/image_width,\n",
    "                                     face_landmarks[INDEX][1]/image_height))\n",
    "        \n",
    "    # Calculate the width and height of the face part.\n",
    "    _, _, width, height = cv2.boundingRect(np.array(landmarks))\n",
    "    \n",
    "    # Retrurn the calculated width, height and the normalized landmarks of the face part.\n",
    "    return width, height, normalized_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2fe46-e4c3-4e4e-bfab-5832e2768077",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Create a Function to Extract Facial Landmarks</font>**\n",
    "\n",
    "Now we will create a function **`extractKeypoints()`**, that will utilize the **`getSize()`** function created above, to calculate size and extract landmarks of different face parts (that add value to differentiate expressions like eyes and mouth, etc.), along with some other useful info (like the distance between eyes and eyebrows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80fd07c2-8227-4a03-9789-583daf64ff60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extractKeypoints_v2(image, face_mesh):\n",
    "    '''\n",
    "    This function will extract the Facial Landmarks (after normalization) of different face parts in an image.\n",
    "    Args:\n",
    "        image:     The input image of the person whose facial landmarks needs to be extracted.\n",
    "        face_mesh: The Mediapipe's face landmarks detection function required to perform the landmarks detection.\n",
    "    Returns:\n",
    "        extracted_landmarks: An array containing the extracted normalized facial landmarks (x and y coordinates).\n",
    "    '''\n",
    "    \n",
    "    # Perform Face landmarks detection.\n",
    "    image, face_landmarks = detectFacialLandmarks(image, face_mesh, draw=False, display=False)\n",
    "    \n",
    "    # Initialize a list to store the extracted landmarks.\n",
    "    extracted_keypoints = []\n",
    "    \n",
    "    # Check if the Face landmarks in the frame are detected.\n",
    "    if len(face_landmarks)>0:\n",
    "        \n",
    "        # Get the width, height, and the landmarks of the face outline.\n",
    "        face_width, face_height, face_outline_landmarks = getSize(image, face_landmarks, \n",
    "                                                                  mp_face_mesh.FACEMESH_FACE_OVAL)\n",
    "\n",
    "        # Get the width, height, and the landmarks of the left and right eye.\n",
    "        left_eye_width, left_eye_height, left_eye_landmarks = getSize(image, face_landmarks, mp_face_mesh.FACEMESH_LEFT_EYE)\n",
    "        right_eye_width, right_eye_height, right_eye_landmarks = getSize(image, face_landmarks, \n",
    "                                                                         mp_face_mesh.FACEMESH_RIGHT_EYE)\n",
    "        \n",
    "        # Get the landmarks of the left and right eyebrow.\n",
    "        _, _, left_eyebrow_landmarks = getSize(image, face_landmarks, mp_face_mesh.FACEMESH_LEFT_EYEBROW)\n",
    "        _, _, right_eyebrow_landmarks = getSize(image, face_landmarks, mp_face_mesh.FACEMESH_RIGHT_EYEBROW)\n",
    "        \n",
    "        # Get the width, height, and the landmarks of the mouth.\n",
    "        mouth_width, mouth_height, mouth_landmarks = getSize(image, face_landmarks, mp_face_mesh.FACEMESH_LIPS)\n",
    "        \n",
    "        # Calculate the center of the left and right eyebrow.\n",
    "        left_eyebrow_center = np.array(left_eyebrow_landmarks).mean(axis=0)\n",
    "        right_eyebrow_center = np.array(right_eyebrow_landmarks).mean(axis=0)\n",
    "        \n",
    "        # Calculate the center of the left and right eye.\n",
    "        left_eye_center = np.array(left_eye_landmarks).mean(axis=0)\n",
    "        right_eye_center = np.array(right_eye_landmarks).mean(axis=0)\n",
    "        \n",
    "        # Calculate the y-coordinate distance from the center of the left and right eyes to the left and right eyebrows respectively.\n",
    "        left_eye_eyebrow_dist =  abs(left_eye_center[1]-left_eyebrow_center[1])\n",
    "        right_eye_eyebrow_dist =  abs(right_eye_center[1]-right_eyebrow_center[1])\n",
    "        \n",
    "        # Extend the face outline landmarks into the list.\n",
    "        extracted_keypoints.extend(face_outline_landmarks)\n",
    "        \n",
    "        # Extend the left and right eyebrow landmarks into the list.\n",
    "        extracted_keypoints.extend(left_eyebrow_landmarks)\n",
    "        extracted_keypoints.extend(right_eyebrow_landmarks)\n",
    "        \n",
    "        # Extend the left and right eye landmarks into the list.\n",
    "        extracted_keypoints.extend(left_eye_landmarks)\n",
    "        extracted_keypoints.extend(right_eye_landmarks)\n",
    "        \n",
    "        # Extend the mouth landmarks into the list.\n",
    "        extracted_keypoints.extend(mouth_landmarks)\n",
    "        \n",
    "        # Extend the different normalized face parts sizes and the distance between eyes and eyebrows into the list.\n",
    "        extracted_keypoints.extend([(left_eye_width/face_width, left_eye_height/face_height),\n",
    "                                    (right_eye_width/face_width, right_eye_height/face_height),\n",
    "                                    (mouth_width/face_width, mouth_height/face_height),\n",
    "                                    (left_eye_eyebrow_dist/face_height, right_eye_eyebrow_dist/face_height)])\n",
    "        \n",
    "    # Convert the list into an float type array.\n",
    "    extracted_keypoints = np.array(extracted_keypoints, dtype=np.float64)\n",
    "    \n",
    "    # Return the extracted normalized facial landmarks.\n",
    "    return extracted_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89213d46-cbb3-49fe-b003-e75f04ad1bf5",
   "metadata": {},
   "source": [
    "Now that we have all the functions we need to extract the landmarks, we can move on to initialize the parameters like the expressions which we want our model to predict and the total number of images from which we want to extract the landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e638b5-653b-4a75-98d7-c3587c8c1087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization Completed.\n"
     ]
    }
   ],
   "source": [
    "# Specify the path where you want to store the landmarks dataset.\n",
    "DATASET_DIR = 'Landmarks'\n",
    "\n",
    "# Check if the directory doesnot already exist.\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "    \n",
    "    # Create the directory.\n",
    "    os.mkdir(DATASET_DIR)\n",
    "\n",
    "# Specify the classes with which we are gonna be working with.\n",
    "expressions = ['neutral', 'happiness', 'anger', 'surprise']\n",
    "\n",
    "# Specify the total number of images for which you want to extract the landmarks.\n",
    "# This must be a multiple of the total number of classes.\n",
    "total_images = 1200\n",
    "\n",
    "# Raise an AssertionError exception, if the total number of images is not a multiple of the number of classes.\n",
    "# This is done to make sure that the equal number of images landmarks for each class are extracted.\n",
    "assert total_images%len(expressions) == 0, f'{total_images} must be a multiple of {len(expressions)}'\n",
    "\n",
    "# Display the success message.\n",
    "print('Initialization Completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f115b-c009-488b-ae26-d7fd62652658",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Data Collection</font>**\n",
    "\n",
    "Now finally, its time to start collecting the data, we will utilize the function **`extractKeypoints_v2()`** to get the required landmarks from the specified number of frames/images of a real-time webcam feed for each expression (on which we want to train our SVM) and store the landmarks dataset into the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f72113c7-c405-4e53-8346-cec023758e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Iterate over the specified classes.\n",
    "for expression in expressions:\n",
    "    \n",
    "    # Iterate over the images indexes for the class (expression), we are iterating upon.\n",
    "    for image_index in range(total_images//len(expressions)):\n",
    "        \n",
    "        # Read a frame.\n",
    "        ok, frame = camera_video.read()\n",
    "        \n",
    "        # Check if frame is not read properly.\n",
    "        if not ok:\n",
    "            \n",
    "            # Subtract 1 from the images indexes continue to the next iteration to read the next frame.\n",
    "            image_index -= 1\n",
    "            continue\n",
    "            \n",
    "        # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Get the height and width of the frame of the webcam video.\n",
    "        frame_height, frame_width, _ = frame.shape\n",
    "        \n",
    "        # Extract the required face keypoints of the person in the frame.\n",
    "        extracted_keypoints = extractKeypoints_v2(frame, face_mesh_videos)\n",
    "        \n",
    "        # Check if the keypoints were not extracted successfully.\n",
    "        if len(extracted_keypoints) == 0:\n",
    "            \n",
    "            # Continue to the next iteration to read the next frame.\n",
    "            continue\n",
    "        \n",
    "        # Flatten the extracted keypoints array.\n",
    "        extracted_keypoints = extracted_keypoints.flatten()\n",
    "        \n",
    "        # Write the current image index and the expression, we are iterating upon on the frame.\n",
    "        cv2.putText(frame, f'{expression.upper()}, Expression Image # {image_index}', (10, frame_height-30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Check if the image index is zero i.e., we are on the first image for an expression.\n",
    "        if image_index == 0: \n",
    "            \n",
    "            # Write the instructions to start collection data on the frame.\n",
    "            cv2.putText(frame, f'Press any key to Start Collecting {expression.upper()} Data.', (10, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0, 255), 4, cv2.LINE_AA)\n",
    "           \n",
    "            # Display the frame.\n",
    "            cv2.imshow('Data Collection', frame)\n",
    "            \n",
    "            # Wait until a key is pressed.\n",
    "            cv2.waitKey(0)\n",
    "        \n",
    "        # Get the directory path inside which we have to store the landmarks for the expression, we are iterating upon.\n",
    "        class_landmarks_dir = os.path.join(DATASET_DIR, expression)\n",
    "        \n",
    "        # Check if the directory doesnot already exist.\n",
    "        if not os.path.exists(class_landmarks_dir):\n",
    "            \n",
    "            # Create the directory.\n",
    "            os.mkdir(class_landmarks_dir)\n",
    "\n",
    "        # Save the extracted landmarks inside a .npy file.\n",
    "        np.save(os.path.join(class_landmarks_dir, str(image_index)), extracted_keypoints)\n",
    "        \n",
    "        # Display the frame.\n",
    "        cv2.imshow('Data Collection', frame)\n",
    "\n",
    "        # Wait for 10ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "        k = cv2.waitKey(10) & 0xFF\n",
    "\n",
    "        # Check if 'ESC' is pressed and break the loop.\n",
    "        if(k == 27):\n",
    "            break\n",
    "                    \n",
    "# Release the VideoCapture Object and close the windows.\n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc9487-2786-4741-a6b8-eccb9ea2efe9",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Load the Dataset</font>**\n",
    "\n",
    "Now that we have the landmarks dataset stored in our disk, we can load the dataset anytime we need. We will utilize the [**`numpy.load()`**](https://numpy.org/doc/stable/reference/generated/numpy.load.html#numpy-load) function to serve the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09764393-85b7-42a2-8969-c7f8e2d0048d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store the landmarks and labels.\n",
    "landmarks, labels = [], []\n",
    "\n",
    "# Iterate over the classes. \n",
    "for class_index, expression in enumerate(expressions):\n",
    "    \n",
    "    # Get the directory path of the expression, we are iterating upon. \n",
    "    expression_dir = os.path.join(DATASET_DIR, expression)\n",
    "    \n",
    "    # Get the names of the files in which the landmarks are stored.\n",
    "    landmarks_files = os.listdir(expression_dir)\n",
    "    \n",
    "    # Iterate over the files names.\n",
    "    for file_index, file_name in enumerate(landmarks_files):\n",
    "        \n",
    "        # Load the landmarks from a .npy file.\n",
    "        image_landmarks = np.load(os.path.join(expression_dir, file_name))\n",
    "        \n",
    "        # Append the landmarks into the list.\n",
    "        landmarks.append(image_landmarks)\n",
    "        \n",
    "        # Append the label into the list.\n",
    "        labels.append(expression)\n",
    "\n",
    "# Display the success message.\n",
    "print(\"Data loaded.\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a7b9c-891d-4451-bfdb-a2afe86a7738",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\"> Split the dataset into Train and Test Set</font>**\n",
    "\n",
    "Now, as you already know from our previous lesson, we need a test set to evaluate our model's performance after training. So now we will split our dataset into train and test subsets randomly using the function [**`sklearn.model_selection.train_test_split()`**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn-model-selection-train-test-split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d28a77f3-4ae6-4f11-b617-33e09c7fa0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into random train and test subsets.\n",
    "train_landmarks, test_landmarks, train_labels, test_labels = train_test_split(landmarks, labels, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16f5ed-e7b1-4583-913b-402dc183bdc1",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Train the Model</font>**\n",
    "\n",
    "Now that we have the dataset ready, we can start training our SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f482c94-c51a-40bb-98a2-74ac476c8702",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "# Initializing the SVM Model.\n",
    "model = svm.SVC(kernel='poly', degree=3, C = 1.0, probability=True)\n",
    "\n",
    "# Start training the model on the training dataset.\n",
    "model.fit(train_landmarks, train_labels)\n",
    "print(\"Training Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd065a7-0642-4d5f-862c-040a3bd2e7d2",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Evaluate the Model</font>**\n",
    "\n",
    "Now, after completing the training process, we can pass the test dataset to the model to evaluate its performance, as we had done in the previous lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24cee458-2ac2-4e30-a861-13a59e730ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Model is 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Get the mean accuracy on the given test data and labels, and display it.\n",
    "score = model.score(test_landmarks, test_labels)\n",
    "print('Accuracy of the Model is {:.2f}%'.format(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca3bb2-d623-4828-b283-ffc6b3896b42",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Save the Model</font>**\n",
    "\n",
    "The evaluation results are quite satisfying, so we can now move on to saving the model into our disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d8d7a27-698c-4dc7-8ce4-e750e640f0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model.\n",
    "pickle.dump(model, open('model/face_expression_v2.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f173795f-5f28-4064-b8a8-ef6e7e002a1b",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Predict Emotions On Real-Time Web-cam Feed</font>**\n",
    "\n",
    "Now let's see how the trained model will perform on a real-time webcam feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97a7b70c-2955-4b2d-8c76-fb5ad148c7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Emotion Recognition', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Load the model from disk.\n",
    "loaded_model = pickle.load(open('model/face_expression_v2.sav', 'rb'))\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "   \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "        \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the height and width of the frame of the webcam video.\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    \n",
    "    # Extract the required face keypoints of the person in the frame.\n",
    "    face_landmarks = extractKeypoints_v2(frame, face_mesh_videos)\n",
    "    \n",
    "    # Check if the keypoints were extracted successfully.\n",
    "    if len(face_landmarks) > 0:\n",
    "        \n",
    "        # Predict the face expression of the person inside the frame.\n",
    "        frame, current_expression = predictEmotion(frame, face_landmarks, loaded_model, threshold=0.8, draw=False, display=False)\n",
    "        \n",
    "        # Write the predicted expression of the person on the frame.\n",
    "        cv2.putText(frame, f'Prediction: {current_expression.upper()}', (10, 30),cv2.FONT_HERSHEY_PLAIN, 2, (0,255,0), 2)\n",
    "    \n",
    "    # Display the frame.\n",
    "    cv2.imshow(\"Emotion Recognition\", frame)\n",
    "    \n",
    "    # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "         \n",
    "# Release the VideoCapture Object and close the windows.\n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Additional comments:\n",
    "#       - In this lesson, we created our own dataset\n",
    "#       - The final outcome has a very impressive prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab19138-811d-4839-ba99-efbe7f7b9bfe",
   "metadata": {},
   "source": [
    "Working pretty well! so the process of collecting the whole dataset from scratch was all worth the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a4955-c002-40cc-8bc2-e78b85772471",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(255,140,0)\"> Code License Agreement </font>**\n",
    "```\n",
    "Copyright (c) 2022 Bleedai.com\n",
    "\n",
    "Feel free to use this code for your own projects commercial or noncommercial, these projects can be Research-based, just for fun, for-profit, or even Education with the exception that you’re not going to use it for developing a course, book, guide, or any other educational products.\n",
    "\n",
    "Under *NO CONDITION OR CIRCUMSTANCE* you may use this code for your own paid educational or self-promotional ventures without written consent from Taha Anwar (BleedAI.com).\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab2c83b243fb879010d169f2f59fe1d865a42357da3e2fb5ab94d633edfe058a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
