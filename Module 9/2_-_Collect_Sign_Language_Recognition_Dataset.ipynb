{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5878de3-fffd-47b0-841e-d731997e4c1d",
   "metadata": {},
   "source": [
    "# **<center><font style=\"color:rgb(100,109,254)\">Module 9: Full-Body Sign Language Recognition</font> </center>**\n",
    "\n",
    "<center>\n",
    "    <img src='https://drive.google.com/uc?export=download&id=1kqMdoDJrt-YxUDPB0YLHcp9f3XAVTRsi' width=800> \n",
    "    <br/>\n",
    "    <a href='https://www.signall.us'>Image Credits</a>\n",
    "</center>\n",
    "    \n",
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\"> Module Outline </font>**\n",
    "\n",
    "The module can be split into the following parts:\n",
    "\n",
    "- *Lesson 1: Introduction to Long Short-Term Memory (LSTM) Networks Theory.*\n",
    "\n",
    "- ***Lesson 2:* Collect Sign Language Recognition Dataset.** *(This Tutorial)*\n",
    "\n",
    "- *Lesson 3:  Train a Sign Language Recognition LSTM Network.*\n",
    "\n",
    "\n",
    "**Please Note**, these Jupyter Notebooks are not for sharing; do read the Copyright message below the Code License Agreement section which is in the last cell of this notebook.\n",
    "-Taha Anwar\n",
    "\n",
    "Alright, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3ff8d9-22b7-4c63-b244-9156c89b2a8e",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\"> Import the Libraries</font>**\n",
    "\n",
    "First, we will import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e11a9f-5342-43ba-b923-23c1e0ceb0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from previous_lesson import detectPoseLandmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a06dc-dc05-49d9-b8ac-5996356a86ba",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Initialize the Pose Detection Model</font>**\n",
    "\n",
    "After that, we will have to initialize the **`mp.solutions.pose`** class and then set up the **`mp.solutions.pose.Pose()`** function with appropriate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb79f841-20f2-415c-8495-b9dcee42505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Set up the pose landmarks function for videos.\n",
    "pose_videos = mp_pose.Pose(static_image_mode=False, model_complexity=1, smooth_landmarks=True, \n",
    "                           enable_segmentation=True, smooth_segmentation=True, \n",
    "                           min_detection_confidence=0.5, min_tracking_confidence=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410b4bd-1d21-412b-a076-bca4388b72fe",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Create a Function to Extract Pose Landmarks</font>**\n",
    "\n",
    "\n",
    "Now we will create a function **`extractPoseKeypoints()`**, that will utilize the function **`detectPoseLandmarks()`** (created in a previous module) to extract the pose landmarks. Remember that, we had converted the Pose landmarks x and y coordinates into their original scale in the function **`detectPoseLandmarks()`**, so now we will have to normalize the coordinates back to the range [0-1], similar to what we had done in the previous module for our face landmarks coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e16aea1-6e03-4820-b062-667080c86918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPoseKeypoints(image, pose):\n",
    "    '''\n",
    "    This function will extract the Pose Landmarks (after normalization) of a person in an image.\n",
    "    Args:\n",
    "        image: The input image of the person whose pose landmarks needs to be extracted.\n",
    "        pose:  The Mediapipe's Pose landmarks detection function required to perform the landmarks detection.\n",
    "    Returns:\n",
    "        extracted_landmarks: A flattened array containing the extracted normalized pose landmarks (x and y coordinates).\n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    # Perform the Pose landmarks detection on the image.\n",
    "    image, pose_landmarks = detectPoseLandmarks(image, pose, draw=True, display=False)\n",
    "    \n",
    "    # Initialize a list to store the extracted landmarks.\n",
    "    extracted_landmarks = []\n",
    "    \n",
    "    # Check if pose landmarks are found. \n",
    "    if len(pose_landmarks) > 0:\n",
    "            \n",
    "        # Iterate over the found pose landmarks. \n",
    "        for landmark in pose_landmarks:\n",
    "            \n",
    "            # Normalize the landmarks and append them into the list.\n",
    "            extracted_landmarks.append((landmark[0]/image_width, landmark[1]/image_height))\n",
    "        \n",
    "    # Convert the list into an array and flatten the array.\n",
    "    extracted_landmarks = np.array(extracted_landmarks).flatten()\n",
    "    \n",
    "    # Return the image and the extracted normalized pose landmarks.\n",
    "    return image, extracted_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211a0ae-5700-4c85-ba7c-ecd65d366951",
   "metadata": {},
   "source": [
    "Now we will initialize the parameters like the signs which we want our model to recognize and the total number of sequences (videos), along with the length of each sequence, from which we want to extract the landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f96d18f-3fac-43b9-b3f4-ffbb3ea7ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization Completed.\n"
     ]
    }
   ],
   "source": [
    "# Specify the classes of which you want to collect data.\n",
    "# Feel free to choose any set of classes.\n",
    "classes_list = [\"Hello\", \"bye\", \"Thankyou\"]\n",
    "\n",
    "# Specify the number of frames of the videos.\n",
    "sequence_length = 30 \n",
    "\n",
    "# Specify the path where you want to store the dataset.\n",
    "DATASET_DIR = 'dataset'\n",
    "\n",
    "# Check if the directory doesnot already exist.\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "    \n",
    "    # Create the directory.\n",
    "    os.mkdir(DATASET_DIR)\n",
    "\n",
    "# Specify the total number of videos for which you want to extract the landmarks.\n",
    "# This must be a multiple of the total number of classes.\n",
    "total_videos = 90\n",
    "\n",
    "# Raise an AssertionError exception, if the total number of videos is not a multiple of the number of classes.\n",
    "# This is done to make sure that the equal number of videos landmarks for each class are extracted.\n",
    "assert total_videos%len(classes_list) == 0, f'{total_videos} must be a multiple of {len(classes_list)}'\n",
    "\n",
    "# Display the success message.\n",
    "print('Initialization Completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737edff1-991d-4c6e-902a-8e49207c2dce",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Data Collection</font>**\n",
    "\n",
    "Now we will start collecting the dataset, we will utilize the function **`extractPoseKeypoints()`** to get the required landmarks for the specified number of sequences for each sign (which we want our sign recognizer to predict) and store the landmarks in the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c01a296-735d-47d2-a169-9f4dfb32b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Data Collection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Iterate over the specified classes.\n",
    "for sign in classes_list:\n",
    "    \n",
    "    # Iterate over the videos indexes for the class (sign), we are iterating upon.\n",
    "    for video_index in range(total_videos//len(classes_list)):\n",
    "        \n",
    "        # Initialize a list to store the video landmarks.\n",
    "        video_landmarks = []\n",
    "        \n",
    "        # Initialize a variable to store the frame counter.\n",
    "        frame_counter = 0\n",
    "                \n",
    "        # Iterate through the video frames.\n",
    "        while frame_counter < sequence_length:\n",
    "            \n",
    "            # Read a frame.\n",
    "            ok, frame = camera_video.read()\n",
    "\n",
    "            # Check if frame is not read properly.\n",
    "            if not ok:\n",
    "                \n",
    "                # Continue to the next iteration to read the next frame.\n",
    "                continue\n",
    "\n",
    "            # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "            frame = cv2.flip(frame, 1)\n",
    "\n",
    "            # Get the height and width of the frame of the webcam video.\n",
    "            frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "            # Extract the required pose keypoints of the person in the frame.\n",
    "            frame, extracted_keypoints = extractPoseKeypoints(frame, pose_videos)\n",
    "            \n",
    "            # Check if the keypoints were not extracted successfully.\n",
    "            if len(extracted_keypoints) == 0:\n",
    "            \n",
    "                # Continue to the next iteration to read the next frame.\n",
    "                continue\n",
    "            \n",
    "            # Write info about the number of frames left of the video and sign that the user have to make.\n",
    "            cv2.putText(frame, f'{sign.upper()}, Sign Video # {video_index}, Frames Left: {sequence_length-frame_counter-1}',\n",
    "                        (10, frame_height-30), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Check if it is the first video and first frame of the sign, we are iterating upon.\n",
    "            if video_index == 0 and frame_counter==0: \n",
    "                \n",
    "                # Write the instructions to start collection data on the frame.\n",
    "                cv2.putText(frame, f'Press any key to Start Collecting {sign.upper()} Sign Data.', (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0, 255), 4, cv2.LINE_AA)\n",
    "                \n",
    "                # Display the frame.\n",
    "                cv2.imshow('Data Collection', frame)\n",
    "                \n",
    "                # Wait until a key is pressed.\n",
    "                cv2.waitKey(0)\n",
    "            \n",
    "            # Display the frame.\n",
    "            cv2.imshow('Data Collection', frame)\n",
    "            \n",
    "            # Wait for 1ms.\n",
    "            cv2.waitKey(1) & 0xFF\n",
    "            \n",
    "            # Append the extracted landmarks into the list.\n",
    "            video_landmarks.append(extracted_keypoints)\n",
    "            \n",
    "            # Increment the frame counter.\n",
    "            frame_counter+=1\n",
    "        \n",
    "        # Get the path to store the video landmarks.\n",
    "        video_landmarks_dir = os.path.join(DATASET_DIR, sign)\n",
    "        \n",
    "        # Check if the directory does not already exist.\n",
    "        if not os.path.exists(video_landmarks_dir):\n",
    "            \n",
    "            # Create the directory.\n",
    "            os.mkdir(video_landmarks_dir)\n",
    "        \n",
    "        # Save the extracted landmarks inside a .npy file.\n",
    "        np.save(os.path.join(video_landmarks_dir, str(video_index)), video_landmarks)\n",
    "        \n",
    "        # Write the instructions to start collection data for the next video. \n",
    "        cv2.putText(frame, f'Press any key to Start Next Video.', (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0, 255), 4, cv2.LINE_AA)\n",
    "        \n",
    "        # Display the frame.\n",
    "        cv2.imshow('Data Collection', frame)\n",
    "        \n",
    "        # Wait until a key is pressed.\n",
    "        # cv2.waitKey(0) \n",
    "                \n",
    "# Release the VideoCapture Object and close the windows.\n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "# Additional comments:\n",
    "#           - This program is for creating a sign language data set\n",
    "#           - This will use the mediapipe solutions to use the sequence of\n",
    "#             landmark movements to detect a sign language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4722f70-a7be-473f-8e0f-2b5e0c909235",
   "metadata": {},
   "source": [
    "Perfect! the dataset is collected and stored successfully in the disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dddd591-9050-40ea-a770-73e7dd6a326d",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(255,140,0)\"> Code License Agreement </font>**\n",
    "```\n",
    "Copyright (c) 2022 Bleedai.com\n",
    "\n",
    "Feel free to use this code for your own projects commercial or noncommercial, these projects can be Research-based, just for fun, for-profit, or even Education with the exception that you’re not going to use it for developing a course, book, guide, or any other educational products.\n",
    "\n",
    "Under *NO CONDITION OR CIRCUMSTANCE* you may use this code for your own paid educational or self-promotional ventures without written consent from Taha Anwar (BleedAI.com).\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab2c83b243fb879010d169f2f59fe1d865a42357da3e2fb5ab94d633edfe058a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
